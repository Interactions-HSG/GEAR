{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7420f93f",
   "metadata": {},
   "source": [
    "# (Pseudo-) Online Feature Calculation\n",
    "\n",
    "This notebook is used to receive data from the HoloLens 2. The data is chunked in 10 second pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a494ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc384884",
   "metadata": {},
   "source": [
    "## Read the data from the newly arrived csv-file and call the feature calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e695e",
   "metadata": {},
   "source": [
    "## Run FeaturesCalculation notebook to make its function accessible here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i FeaturesCalculation.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25237aa8",
   "metadata": {},
   "source": [
    "## Calculate the features and save them as csv\n",
    "See `FeaturesCalculation.ipynb` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features_for_10s_chunk(newdf):\n",
    "    list_of_features = []\n",
    "    newdf_valid = only_valid_data(newdf)\n",
    "    df_fixations = get_fixation_df(newdf_valid)\n",
    "    features = calculate_fixation_features(df_fixations, 10)\n",
    "    blinks = calculate_blink_features(newdf,10)\n",
    "    directions = calculate_directions_of_list(df_fixations) \n",
    "    density = calculate_fixation_density(newdf_valid, df_fixations)\n",
    "    features.update(blinks)\n",
    "    features.update(directions)\n",
    "    features.update(density)\n",
    "    features[\"label\"] = \"\"\n",
    "    features[\"duration\"] = \"10\"\n",
    "    # change the participant ID here, when the participant changes\n",
    "    features[\"participant_id\"] = \"001\"            \n",
    "    list_of_features.append(features)  \n",
    "    flat_ls = [item for sublist in list_of_features for item in sublist]\n",
    "    # change the participant ID here, when the participant changes\n",
    "    feature_file_path = save_as_csv(list_of_features, \"001\", './OnlineFeatureFiles/')\n",
    "    return feature_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_features(gaze_data_file_path):\n",
    "    # continue only if the csv-file contains data\n",
    "    if os.stat(gaze_data_file_path).st_size != 0:\n",
    "        df = pd.read_csv(gaze_data_file_path)\n",
    "        feature_file_path = calculate_features_for_10s_chunk(df)\n",
    "        print(f\"Feature calculation done for: {gaze_data_file_path}\")\n",
    "        print(f\"Feature file path: {feature_file_path}\")\n",
    "        return feature_file_path\n",
    "# csv_to_features(\"./HL2_DataCollection/2022_09_23-13_41_19-Alex01-Inspection02.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e6fc90",
   "metadata": {},
   "source": [
    "## Run SVM notebook to make its function accessible here\n",
    "If you are not training on normalized data this make take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528de9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i SVM_Test_On_HL2_Data_.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682dbc1",
   "metadata": {},
   "source": [
    "## Predict the class for the last arrived data chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_values(df): \n",
    "    scaler = MaxAbsScaler()\n",
    "    training_features_plus_new_row = pd.concat([features, df])  # features -> contains all the features for training and testing\n",
    "    scaler.fit(training_features_plus_new_row)\n",
    "    scaled = scaler.transform(training_features_plus_new_row)\n",
    "    scaled_features = pd.DataFrame(scaled, columns=df.columns)\n",
    "    row_for_last_chunk = scaled_features.tail(1)\n",
    "    print(\"Normalized Features:\")\n",
    "    display(row_for_last_chunk)\n",
    "    return row_for_last_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counter = 0\n",
    "classes = [\"Inspection\", \"Reading\", \"Search\"]\n",
    "kernels = [linear, poly, rbf, sig]\n",
    "def predict_class_for_last_chunk(feature_file_path):\n",
    "    if os.stat(feature_file_path).st_size != 0:\n",
    "        df = pd.read_csv(feature_file_path)\n",
    "        cur_number_of_rows = df.shape[0]\n",
    "        # only continue if we have a new row\n",
    "        if cur_number_of_rows > row_counter:\n",
    "            last_row = df.tail(1)\n",
    "            display(last_row)\n",
    "            \n",
    "            lr_features= last_row[[\"meanFix\", \"maxFix\", \"varFix\", \"xDir\", \"yDir\", \"fixDensPerBB\"]]\n",
    "            last_row = normalize_values(lr_features)\n",
    "            \n",
    "            classes = linear.classes_\n",
    "            print(f\"\\n---\\nclasses: {classes}\")\n",
    "            \n",
    "            results = {}\n",
    "            \n",
    "            for kernel in kernels:\n",
    "                new_pred_prob = kernel.predict_proba(last_row)\n",
    "                new_pred_class = kernel.predict(last_row)\n",
    "                # print(\"\\n---\\nNew Prediction:\")\n",
    "                display(new_pred_prob)\n",
    "                display(new_pred_class)\n",
    "                pred_list = new_pred_prob.tolist()[0]\n",
    "                max_prob = max(pred_list)\n",
    "                # print(f\"max_prob: {max_prob}\")\n",
    "                max_index = pred_list.index(max_prob)\n",
    "                # print(f\"max_index: {max_prob}\")\n",
    "                results[kernel.kernel] = {\"pred_class\": new_pred_class[0], \"max_prob\": max_prob, \"max_index\": max_index }\n",
    "                # results[kernel.kernel] = [new_pred_class[0], max_prob, max_index ]\n",
    "                \n",
    "            display(results)\n",
    "        \n",
    "            \n",
    "            max_proba = {max(float(d['max_prob']) for d in results.values())}\n",
    "            key = [i for i in results if results[i]['max_prob']==max_proba]\n",
    "            # print(f\"max_proba: {max_proba}, key: {key}\")\n",
    "            predicts = [d['pred_class'] for d in results.values()]\n",
    "            final_class_from_predict = max(set(predicts), key = predicts.count)\n",
    "            # print(f\"final_class_from_predict: {final_class_from_predict}\")\n",
    "            \n",
    "            \n",
    "            new_linear_pred_prob = linear.predict_proba(last_row)\n",
    "            new_linear_pred_class = linear.predict(last_row)\n",
    "            # print(\"\\n---\\nNew Linear Prediction:\")\n",
    "            # display(new_linear_pred_prob)\n",
    "            # display(new_linear_pred_class)\n",
    "            lin_list = new_linear_pred_prob.tolist()[0]\n",
    "            lin_max_prob = max(lin_list)\n",
    "            # print(f\"lin_max_prob: {lin_max_prob}\")\n",
    "            lin_max_index = lin_list.index(lin_max_prob)\n",
    "            # print(f\"lin_max_index: {lin_max_index}\")\n",
    "\n",
    "            new_poly_pred_prob = poly.predict_proba(last_row)\n",
    "            new_poly_pred_class = poly.predict(last_row)\n",
    "            # print(\"\\n---\\nNew Poly Prediction:\")\n",
    "            # display(new_poly_pred_prob)\n",
    "            # display(new_poly_pred_class)\n",
    "            poly_list = new_poly_pred_prob.tolist()[0]\n",
    "            poly_max_prob = max(poly_list)\n",
    "            # print(f\"poly_max_prob: {poly_max_prob}\")\n",
    "            poly_max_index = poly_list.index(poly_max_prob)\n",
    "            # print(f\"poly_max_index: {poly_max_index}\")\n",
    "\n",
    "            new_rbf_pred_prob = rbf.predict_proba(last_row)\n",
    "            new_rbf_pred_class = rbf.predict(last_row)\n",
    "            # print(\"\\n---\\nNew RBF Prediction:\")\n",
    "            # display(new_rbf_pred_prob)\n",
    "            # display(new_rbf_pred_class)\n",
    "            rbf_list = new_rbf_pred_prob.tolist()[0]\n",
    "            rbf_max_prob = max(rbf_list)\n",
    "            # print(f\"rbf_max_prob: {rbf_max_prob}\")\n",
    "            rbf_max_index = rbf_list.index(rbf_max_prob)\n",
    "            # print(f\"rbf_max_index: {rbf_max_index}\")\n",
    "            \n",
    "\n",
    "            new_sig_pred_prob = sig.predict_proba(last_row)\n",
    "            new_sig_pred_class = sig.predict(last_row)\n",
    "            # print(\"\\n---\\nNew Sig Prediction:\")\n",
    "            # display(new_sig_pred_prob)\n",
    "            # display(new_sig_pred_class)\n",
    "            sig_list = new_sig_pred_prob.tolist()[0]\n",
    "            sig_max_prob = max(sig_list)\n",
    "            # print(f\"sig_max_prob: {sig_max_prob}\")\n",
    "            sig_max_index = sig_list.index(sig_max_prob)\n",
    "            # print(f\"sig_max_index: {sig_max_index}\")\n",
    "            \n",
    "            print(\"----------------\")\n",
    "            max_probs = { lin_max_prob: lin_max_index, poly_max_prob: poly_max_index,\n",
    "                        rbf_max_prob: rbf_max_index}\n",
    "            \n",
    "            max_prediction_value = max(max_probs, key=float)\n",
    "            # print(max_prediction_value)\n",
    "            final_max_index = max_probs[max_prediction_value]\n",
    "            final_class_from_probs = classes[final_max_index]\n",
    "            \n",
    "            print(f\"final_class_from_probs: {final_class_from_probs}\")\n",
    "            \n",
    "            predicts = [new_linear_pred_class[0], new_poly_pred_class[0], new_rbf_pred_class[0]]\n",
    "            final_class_from_predict = max(set(predicts), key = predicts.count)\n",
    "            print(f\"final_class_from_predict: {final_class_from_predict}\")\n",
    "            \n",
    "            \n",
    "            same_class_predicted = (final_class_from_probs == final_class_from_predict)\n",
    "            print(f\"same_class_predicted: {same_class_predicted}\")\n",
    "            \n",
    "            if (same_class_predicted and max_prediction_value > 0.5):\n",
    "                # send request to HL2 with class and probability\n",
    "                send_activity(final_class_from_probs, max_prediction_value)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd846c",
   "metadata": {},
   "source": [
    "## Send the recognized activity back to the HooLens 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def send_activity(activity, probability):\n",
    "\n",
    "    # insert the HoloLens 2's IP address here\n",
    "    holo_url = \"http://0.0.0.0:5000\"\n",
    "\n",
    "    url = \"{}/?activity={}&probability={}\".format(str(holo_url), str(activity), str(probability))\n",
    "    print(url)\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, timeout=120)\n",
    "        print(r)\n",
    "        if r.status_code == 200:\n",
    "            print(\"Notified Hololens about activity {}\".format(activity))\n",
    "        else:\n",
    "            print(\"Request {} failed with status code {}\".format(url, r.status_code))\n",
    "    except requests.ConnectionError as e:\n",
    "        print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "        print(str(e))\n",
    "    except requests.Timeout as e:\n",
    "        print(\"OOPS!! Timeout Error\")\n",
    "        print(str(e))\n",
    "    except requests.RequestException as e:\n",
    "        print(\"OOPS!! General Error\")\n",
    "        print(str(e))\n",
    "\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def create_and_send_test_data():\n",
    "    activities = [\"reading\", \"writing\", \"searching\", \"inspecting\"]\n",
    "    a = random.randint(0, 3)\n",
    "    activity = activities[a]\n",
    "    confidence = random.uniform(0, 1)\n",
    "    print(f\"{activity}: {confidence}\")\n",
    "    send_activity(activity, confidence)\n",
    "\n",
    "\n",
    "def sender():\n",
    "    start_time = time.time()\n",
    "    interval = 5\n",
    "    for i in range(20):\n",
    "        time.sleep(start_time + i * interval - time.time())\n",
    "        create_and_send_test_data()\n",
    "        print(\"sent data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9268036",
   "metadata": {},
   "source": [
    "## Run a simple Flask server that receives the raw gaze data from the HL2\n",
    "Make sure to enter your computer's IP-address as host and the correct port!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d07b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/', methods=['POST', 'PUT'])\n",
    "def result():\n",
    "    \n",
    "    new_csv = request.files[\"gazedata\"].read()\n",
    "    filename = request.form[\"filename\"]\n",
    "    print(f\"filename: {filename}\")\n",
    "    filepath = os.path.join(\"./OnlineGazeDataChunks/\", filename)\n",
    "    print(f\"filepath: {filepath}\")\n",
    "    outF = open(filepath, \"wb\")\n",
    "    outF.write(new_csv)\n",
    "    feature_file_path = csv_to_features(filepath)\n",
    "    predict_class_for_last_chunk(feature_file_path)\n",
    "    return 'Received !'  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # insert the IP address here to which the Unity app sends the gaze data \n",
    "    # (i.e. the public address of the computer this file runs on)\n",
    "    app.run(host='localhost', port=5555)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a7333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb29fbf1579f192e317de19bbdbd42ba1cee731e6ebb339657c7fa01a6f49f1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
