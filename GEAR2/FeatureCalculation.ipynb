{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Feature Calculation\n",
    "\n",
    "__UbiComp Assignment 02, Task 01:__\n",
    "This second notebook allows you to calculate selected features and corresponding labels that are needed for the classification of gaze data.\n",
    "\n",
    "Parts of this code are taken from a tutorial from Pupil Labs. See Section 3 in their [repository](https://github.com/pupil-labs/pupil-tutorials/blob/master/01_load_exported_data_and_visualize_pupillometry.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dispersion-Threshold Identification I-DT algorithm\n",
    "\n",
    "We start with defining some threshold parameters needed for detecting the fixations, as described in [Salvucci and Goldberg 2000](https://dl.acm.org/doi/abs/10.1145/355017.355028)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# spatial threshold or the dispersion itself\n",
    "max_dispersion = np.deg2rad(1.6)\n",
    "# temporal threshold or duration\n",
    "min_duration = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Some auxilary functions for the fixation calculation from raw gaze data\n",
    "\n",
    "Now we need several functions that help us detecting the fixations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vector Dispersion\n",
    "This function (from the Pupil Labs Tutorial) calculates the dispersion of a given list of (gaze) vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial.distance\n",
    "\n",
    "def vector_dispersion(vectors):\n",
    "    distances = scipy.spatial.distance.pdist(vectors, metric='cosine')\n",
    "    distances.sort()\n",
    "    cut_off = np.max([distances.shape[0] // 5, 4])\n",
    "    return np.arccos(1. - distances[-cut_off:].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dispersion Calculation\n",
    "We adapted the following function (also from the Pupil Labs Tutorial) so that it works with the Microsft HoloLens2 gaze data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gaze_dispersion(eye_data):\n",
    "    base_data = eye_data\n",
    "\n",
    "    vectors = []\n",
    "    for p in base_data:\n",
    "        vectors.append((p['gazeDirection_x'], p['gazeDirection_y'], p['gazeDirection_z']))\n",
    "    vectors = np.array(vectors, dtype=np.float32)\n",
    "\n",
    "    if len(vectors) < 2:\n",
    "        return float(\"inf\")\n",
    "    else:\n",
    "        return vector_dispersion(vectors)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Centroid Calculation\n",
    "The following function calculates the centroid of each group of points that is identified as a fixation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid(eye_data):\n",
    "    '''Calculates the centroid for each point in a df of points.\n",
    "    Input: Df of points.\n",
    "    Output: Vector containg the centroid of all points.'''\n",
    "    x = [p['gazeDirection_x'] for p in eye_data]\n",
    "    y = [p['gazeDirection_y'] for p in eye_data]\n",
    "    z = [p['gazeDirection_z'] for p in eye_data]\n",
    "    return (sum(x) / len(eye_data), sum(y) / len(eye_data), sum(z) / len(eye_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Fixation Detection\n",
    "The following function calculates the fixations (also adapted from the Pupil Labs Tutorial).\n",
    "\n",
    "We changed the column names, added centroid calculation, and changed the output format to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def detect_fixations(gaze_data):\n",
    "    # Convert Pandas data frame to list of Python dictionaries\n",
    "    gaze_data = gaze_data.T.to_dict().values()\n",
    "\n",
    "    candidate = deque()\n",
    "    future_data = deque(gaze_data)\n",
    "    while future_data:\n",
    "        # check if candidate contains enough data\n",
    "        if len(candidate) < 2 or candidate[-1]['eyeDataTimestamp'] - candidate[0]['eyeDataTimestamp'] < min_duration:\n",
    "            datum = future_data.popleft()\n",
    "            candidate.append(datum)\n",
    "            continue\n",
    "\n",
    "        # Minimal duration reached, check for fixation\n",
    "        dispersion = gaze_dispersion(candidate)\n",
    "        if dispersion > max_dispersion:\n",
    "            # not a fixation, move forward\n",
    "            candidate.popleft()\n",
    "            continue\n",
    "\n",
    "        # Minimal fixation found. Try to extend!\n",
    "        while future_data:\n",
    "            datum = future_data[0]\n",
    "            candidate.append(datum)\n",
    "\n",
    "            dispersion = gaze_dispersion(candidate)\n",
    "            if dispersion > max_dispersion:\n",
    "                # end of fixation found\n",
    "                candidate.pop()\n",
    "                break\n",
    "            else:\n",
    "                # still a fixation, continue extending\n",
    "                future_data.popleft()\n",
    "        centroid = get_centroid(candidate)\n",
    "        yield {\"start\": candidate[0]['eyeDataTimestamp'], \"end\": candidate[-1]['eyeDataTimestamp'],\n",
    "               \"duration\": candidate[-1]['eyeDataTimestamp'] - candidate[0]['eyeDataTimestamp'],\n",
    "              \"centroid\": centroid, \"dispersion\": dispersion}\n",
    "        candidate.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculation of Gaze Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Valid Data\n",
    "Input: Raw gaze data from the Microsoft Hololens2 (through the framework [ARETT](https://github.com/AR-Eye-Tracking-Toolkit/ARETT)).\\\n",
    "Output: Only valid gaze points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def only_valid_data(data):\n",
    "    '''Returns only valid gaze points. Those have values in gazeDirection_x etc.'''\n",
    "    return data[(data.gazeHasValue == True) & (data.isCalibrationValid == True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Blink calculation\n",
    "\n",
    "Whenever `gazeHasValue == False` we assume it's a blink. Consequetive rows without data (i.e.  `gazeHasValue == False`) are counted as one single blink.\n",
    "\n",
    "We calculate __five features__ based on the blinks: \n",
    "- number of blinks\n",
    "- mean duration of blinks\n",
    "- maximum duration of blinks\n",
    "- minimum duration of blinks\n",
    "- blink rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def calculate_blink_features(df, timespan): \n",
    "    ''' Calculates the blink features for a given df of raw data.\n",
    "    Input: Dataframe with raw data (incl. invalid points), timespan of data chunk in seconds\\\n",
    "    Output: Dict with the blink features\n",
    "    '''\n",
    "    i=0;\n",
    "    blink_list = []\n",
    "    blink_duration_list = []\n",
    "    number_of_blinks = 0\n",
    "    window_start_time = df[\"eyeDataTimestamp\"][0]\n",
    "    window_end_time = df[\"eyeDataTimestamp\"][0]\n",
    "    all_false = 0;\n",
    "    if (not window_start_time  or not window_end_time):\n",
    "        return {}\n",
    "        \n",
    "    for i, row in df.iterrows():    \n",
    "        \n",
    "        cur_number_of_blinks = 0\n",
    "        if (not row[\"gazeHasValue\"]):\n",
    "            cur_number_of_blinks+=1\n",
    "            all_false += 1\n",
    "            while (i < len(df[\"gazeHasValue\"])) and (not df[\"gazeHasValue\"][i]) and  window_end_time - window_start_time < 1000:\n",
    "                window_end_time = row[\"eyeDataTimestamp\"]\n",
    "                i+=1\n",
    "                all_false +=1\n",
    "            blink_list.append(cur_number_of_blinks)\n",
    "            duration = window_end_time - window_start_time\n",
    "            blink_duration_list.append(duration)\n",
    "        \n",
    "        number_of_blinks += cur_number_of_blinks;\n",
    "        if  window_end_time - window_start_time > 1000:\n",
    "            window_start_time = window_end_time\n",
    "    \n",
    "    blinks_per_second = 0\n",
    "    if (len(blink_list)  > 0):\n",
    "        blinks_per_second = number_of_blinks / timespan \n",
    "    avg_blink_duration = 0\n",
    "    min_blink_duration = 0\n",
    "    max_blink_duration = 0\n",
    "    if (len(blink_duration_list) > 0):\n",
    "        avg_blink_duration = sum(blink_duration_list) / len(blink_duration_list)\n",
    "        min_blink_duration = min(blink_duration_list)\n",
    "        max_blink_duration = max(blink_duration_list)\n",
    "        \n",
    "    # print(\"all_blinks: \", number_of_blinks, \" avg_blink_duration: \", avg_blink_duration, \n",
    "    #    \" min_blink_duration: \", min_blink_duration, \" max_blink_duration: \", max_blink_duration,\n",
    "    #  \" blinks_per_second: \", blinks_per_second, \" all false: \", all_false)\n",
    "    \n",
    "    return {\"number_of_blinks\":number_of_blinks, \"blinkMean\": avg_blink_duration, \n",
    "            \"blinkMin\": min_blink_duration, \"blinkMax\": max_blink_duration, \n",
    "            \"blinkRate\": blinks_per_second}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Calculate Fixation Features\n",
    "The following function calculates __11 features__ based on the fixations. \n",
    "We need the following features:\n",
    "- minimal fixation duration\n",
    "- maximal fixation duration\n",
    "- mean fixation duration\n",
    "- variance of the fixation duration\n",
    "- standard deviation of the fixation duration\n",
    "- minimal fixation dispersion\n",
    "- maximal fixation dispersion\n",
    "- mean fixation dispersion\n",
    "- variance of the fixation dispersion\n",
    "- standard deviation of the fixation dispersion\n",
    "- fixation frequency per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fixation_features(df_fixations, timespan):\n",
    "    '''Calculates the fixation features. \n",
    "    Input: Dataframe with fixation, timespan of data chunk in seconds.\n",
    "    Output: Dict containing the fixation features.'''\n",
    "\n",
    "    min_fix = df_fixations[\"duration\"].min()\n",
    "    max_fix = df_fixations[\"duration\"].max()\n",
    "    mean_fix = df_fixations[\"duration\"].mean()\n",
    "    var_fix = df_fixations[\"duration\"].var()\n",
    "    std_fix = df_fixations[\"duration\"].std()\n",
    "    \n",
    "    min_dispersion = df_fixations[\"dispersion\"].min()\n",
    "    max_dispersion = df_fixations[\"dispersion\"].max()\n",
    "    mean_dispersion = df_fixations[\"dispersion\"].mean()\n",
    "    var_dispersion = df_fixations[\"dispersion\"].var()\n",
    "    std_dispersion = df_fixations[\"dispersion\"].std()\n",
    "    \n",
    "    fixation_frequency_second = (len(df_fixations[\"dispersion\"]) / timespan)\n",
    "    \n",
    "    # print(\"min: \", min_fix, \" max: \", max_fix, \" mean: \", mean_fix, \" var: \", var_fix, \"std: \", std_fix)\n",
    "    # print(\"min dispersion: \", min_dispersion, \" max: \", max_dispersion, \" mean: \", mean_dispersion, \n",
    "    #      \" var: \", var_dispersion)\n",
    "    #print(\"x dispersion: \", dispersion_x, \" y dispersion: \", dispersion_y, \" z dispersion: \", dispersion_z)\n",
    "    \n",
    "    return {\"meanFix\": mean_fix, \"minFix\": min_fix, \"maxFix\": max_fix, \"varFix\": var_fix, \"stdFix\": std_fix,\n",
    "            \"meanDis\": mean_dispersion, \"minDis\": min_dispersion, \"maxDis\": max_dispersion,\n",
    "            \"varDis\": var_dispersion, \"stdDisp\": std_dispersion,\n",
    "            \"freqDisPerSec\": fixation_frequency_second}\n",
    "    \n",
    "\n",
    "def get_fixation_df(df_valid):\n",
    "    '''Calls function to calculate Fixations. Converts the list of fixations to a dataframe and numbers the rows as index.\n",
    "     Input: Dataframe containg valid gaze points.\n",
    "     Output: Dataframe containing the fixation features.'''\n",
    "    fixations = list(detect_fixations(df_valid))\n",
    "    df = pd.DataFrame(fixations)\n",
    "    df['index'] = range(1, len(df) + 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Direction of the Succesive Fixation Points\n",
    "\n",
    "This function calculates the dominant direction along the __x__ and __y__ axis (i.e., __two addtional features__).\n",
    "\n",
    "First for each x- and y-value it is calulated whether it is larger then the point in the list before (the points are ordered by time).\n",
    "Then all the True-values in this list are summed up and divided by the length of the list.\n",
    "\n",
    "As a result, e.g., if (xDir == 1), then each x-value is further right than its predecessors in the list.\n",
    "\n",
    "If xDir > 0.5 the x-direction is (mostly) to the right. \\\n",
    "If xDir = 0.5 the x-direction is equally to both sides. \\\n",
    "If xDir < 0.5 the x-direction is (mostly) to the left. \\\n",
    "If yDir > 0.5 the y-direction is (mostly) to the top. \\\n",
    "If yDir = 0.5 the y-direction is equally to both sides. \\\n",
    "If yDir < 0.5 the y-direction is (mostly) to the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_directions_of_list(points):\n",
    "    '''Calculates the dominant direction of points.\n",
    "    Input: Dataframe containing fixation points.\n",
    "    Output: Dict with dominant direction for x (xDir) and y (yDir).\n",
    "    '''\n",
    "    x_values, y_values, z_values = zip(*points['centroid'])\n",
    "    # Get a list of whether a given value is greater then the previous one in the list\n",
    "    res_x = [float(val1) < float(val2) for val1, val2 in zip(x_values, x_values[1:])]\n",
    "    # Sum all that are True\n",
    "    sum_x = sum(res_x)\n",
    "    # Divide the sum by the total number of values to get the desired output.\n",
    "    # dir_x is -1 if there are no fixation (i.e. prevent division by zero)\n",
    "    dir_x = -1\n",
    "    if len(res_x) != 0:\n",
    "        dir_x = sum_x/len(res_x)\n",
    "        \n",
    "    res_y = [float(val1) < float(val2) for val1, val2 in zip(y_values, y_values[1:])]\n",
    "    sum_y = sum(res_y)\n",
    "    dir_y = -1\n",
    "    if len(res_y) != 0:\n",
    "        dir_y = sum_y /len(res_y)\n",
    "      \n",
    "    \n",
    "    return {\"xDir\": dir_x, \"yDir\": dir_y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Fixation Density\n",
    "The number of fixations per area (i.e., the area from where all gaze points are collected). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fixation_density(df_all, df_fix):\n",
    "    '''Calculates the fixation density per area.\n",
    "    Input: Dataframe with all valid gazepoints, Dataframe with fixations.\n",
    "    Output: Dict containing the fixation density.'''\n",
    "    min_x = df_all['gazeDirection_x'].min()\n",
    "    min_y = df_all['gazeDirection_y'].min()\n",
    "    max_x = df_all['gazeDirection_x'].max()\n",
    "    max_y = df_all['gazeDirection_y'].max()\n",
    "    \n",
    "    length = abs(max_x-min_x)\n",
    "    height = abs(max_x-min_x)\n",
    "    area = length*height\n",
    "    \n",
    "    number_of_fixations = len(df_fix)\n",
    "    \n",
    "    fix_dens = -1\n",
    "    if area != 0: \n",
    "        fix_dens = number_of_fixations/area\n",
    "    return {\"fixDensPerBB\": fix_dens}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Get Features for a given *timespan* (in seconds)\n",
    "This function makes use of the feature calculations explained above.\\\n",
    "It splits the data into chunks based on a given timespan and discards the final chunk if its duration is shorter than the *timespan*.\\\n",
    "Finally, it returns the feature-chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tabulate\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def get_features_for_n_seconds(df, timespan, label, participant_id):\n",
    "    ''' Calculates the features for a raw gaze data in chunks of n seconds.\n",
    "    Input: Dataframe with raw gaze data, timespan to chunk, label (i.e. activity class).\n",
    "    Output: List of dictionaries, one dictionary contains a chunk of features.\n",
    "    '''\n",
    "    \n",
    "    list_of_features = []\n",
    "    i = 0\n",
    "    while i < len(df)-1: \n",
    "        newdf = pd.DataFrame(columns=df.columns)\n",
    "        start_time = df[\"eyeDataTimestamp\"][i]  \n",
    "        \n",
    "        while i < len(df)-1 and df[\"eyeDataTimestamp\"][i] < (start_time+timespan*1000):\n",
    "            entry = df.iloc[[i]]\n",
    "            newdf = pd.concat([newdf,entry])\n",
    "            i+=1\n",
    "        newdf.reset_index(inplace = True)\n",
    "        \n",
    "        if (len(newdf) > timespan*28):\n",
    "            newdf_valid = only_valid_data(newdf)\n",
    "            df_fixations = get_fixation_df(newdf_valid)\n",
    "            \n",
    "            features = calculate_fixation_features(df_fixations, timespan)\n",
    "            blinks = calculate_blink_features(newdf,timespan)\n",
    "\n",
    "            directions = calculate_directions_of_list(df_fixations)            \n",
    "            density = calculate_fixation_density(newdf_valid, df_fixations)\n",
    "            \n",
    "            features.update(blinks)\n",
    "            features.update(directions)\n",
    "            features.update(density)\n",
    "            features[\"label\"] = label\n",
    "            # print(f\"label: {label}\")\n",
    "            features[\"duration\"] = timespan\n",
    "            features[\"participant_id\"] = participant_id            \n",
    "            list_of_features.append(features)   \n",
    "        \n",
    "    return list_of_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Assemble the Features and Write them to a CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Save the features in a CSV file\n",
    "\n",
    "In offline processing (i.e., with the previously collected gaze data) we write to the csv file (*w*).\\\n",
    "In online processing (i.e., gaze data collected live from an eye tracker), we append to the csv file (*a*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabulate\n",
    "import csv\n",
    "def save_as_csv(list_of_dict, participant, folder):\n",
    "    '''Saves a list of dicts as one csv file.\n",
    "    Input: List of feature dicts, participant id.\n",
    "    Output: CSV file, saved in same directory as this file.'''\n",
    "    header = list_of_dict[0].keys()\n",
    "    rows =  [x.values() for x in list_of_dict]\n",
    "\n",
    "    keys = list_of_dict[0].keys()\n",
    "    file_name = os.path.join(folder, f'feature_list_P{participant}.csv')\n",
    "    # if the file already exists, append the rows\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, 'a', newline='') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, keys)\n",
    "            dict_writer.writerows(list_of_dict)\n",
    "    # otherwise create a new file\n",
    "    else:\n",
    "        with open(file_name, 'w', newline='') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, keys)\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(list_of_dict)\n",
    "    # '''        \n",
    "    file_name_all = os.path.join(folder, f'feature_list_all.csv')\n",
    "    # if the file already exists, append the rows\n",
    "    if os.path.exists(file_name_all):\n",
    "        with open(file_name_all, 'a', newline='') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, keys)\n",
    "            dict_writer.writerows(list_of_dict)\n",
    "    # otherwise create a new file\n",
    "    else:\n",
    "        with open(file_name_all, 'w', newline='') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, keys)\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(list_of_dict)\n",
    "    # '''\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Collect the data from csv files\n",
    "This function reads gaze data from all csv files in one folder. \n",
    "Based on the filename, it extracts the activity and the participant ID. \n",
    "\n",
    "You need to put the data in a folder that is defined by the variable `root_dir` or change the path accordingly.\n",
    "The filenames need to be in this form: *00_reading.csv*, where the number denotes the particpant ID and the second part the activity.\n",
    "\n",
    "The output dict has this form:\n",
    "```\n",
    "{ \"01\" :\n",
    "    { \"reading\" : \"./Data/RawGazeData/00_reading.csv\",\n",
    "      \"search\" : ...\n",
    "    },\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def collect_data_from_csv_files():\n",
    "    ''' Collects the filenames of all csv-files in a given folder.\n",
    "    Input: -\n",
    "    Output: Dict containing the activites per participants.'''\n",
    "    root_dir = \"./Data/RawGazeData/\"\n",
    "    \n",
    "    files_list = os.listdir(root_dir)\n",
    "    print(files_list)\n",
    "    # filenames are like this: 00_reading.csv, 01_reading.csv,...\n",
    "    # where the \"00\" etc. indicates the participant number\n",
    "    df_files = {}\n",
    "    for index, path in enumerate(files_list):\n",
    "        if (\"csv\" in path):\n",
    "            name = path.split(\"_\")\n",
    "            participant_id = name[0]\n",
    "            activity = name[1].split(\".\")[0]\n",
    "            if df_files.get(participant_id):\n",
    "                df_files[participant_id][activity] = f\"{root_dir}{path}\"\n",
    "            else:\n",
    "                df_files.update({participant_id: {activity: f\"{root_dir}{path}\"}})\n",
    "    return df_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Prepare the Features for Classification\n",
    "It starts the calculation of features by calling the functions from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features_and_save_for_list_of_files():\n",
    "    '''Calculates the features for multiple gaze data files and saves those as CSV files.\n",
    "    Input: Dict containg the activity class (key) and the paths to the gaze data files (value)\n",
    "    Output: Feature-files.'''\n",
    "    \n",
    "    paths = collect_data_from_csv_files()\n",
    "    for participant_id, participant_item in paths.items():\n",
    "        feature_list = []\n",
    "        for activity, path in participant_item.items():\n",
    "            print(f\"calculating features for : {path}\")\n",
    "            df = pd.read_csv(path)\n",
    "            print(f\"activity: {activity}\")\n",
    "            feature_list.append(get_features_for_n_seconds(df, 20, activity, participant_id))\n",
    "\n",
    "        flat_ls = [item for sublist in feature_list for item in sublist]\n",
    "        # change the folder here to not overwrite the data we provided!\n",
    "        save_as_csv(flat_ls, participant_id, './Data/FeatureFiles/')\n",
    "        \n",
    "    print(\"done.\")  \n",
    "\n",
    "# Uncomment this line to calculate the features from the raw gaze data.\n",
    "# calculate_features_and_save_for_list_of_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed47d88ac7354fe37187b37981a417a5ac4c5f036a7852c056cd591cda01fe8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
